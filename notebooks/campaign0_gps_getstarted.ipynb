{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitcondacadf76f2c1054d4581372baf88a91024",
   "display_name": "Python 3.6.9 64-bit (conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gps_path = '../gpssamples'\n",
    "gpx_file = '3145554.gpx'\n",
    "kml_file = '19022020_Adour_6.kml'\n",
    "gpx_path = os.path.join(gps_path,gpx_file)\n",
    "kml_path = os.path.join(gps_path,kml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/gpxpy/\n",
    "import gpxpy\n",
    "import gpxpy.gpx \n",
    "gpx_file = open(gpx_path,'r',encoding='utf-8')\n",
    "gpx = gpxpy.parse(gpx_file) # data from parsed gpx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' gpsPointList function extract gps points from gpx file '''\n",
    "''' gpxdata is a gpxpy object that returns data from a parsed gpx file'''\n",
    "''' gpsPointList return a list of dictionnary points with Time, Lat, Long, Elev'''\n",
    "def gpsPointList(gpxdata):\n",
    "    point_list = []\n",
    "    for track in gpxdata.tracks:\n",
    "        for segment in track.segments: \n",
    "            for point in segment.points:\n",
    "                point_info = {'Time':point.time,'Latitude':point.latitude,'Longitude':point.longitude,'Elevation':point.elevation}\n",
    "                point_list.append(point_info)\n",
    "    return point_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'Time': datetime.datetime(2019, 11, 9, 13, 18, 27, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.512632940826514, 'Longitude': -1.287282460550814, 'Elevation': 69.95082215685397}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 27, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.512609944758175, 'Longitude': -1.2872313093696484, 'Elevation': 59.431979357264936}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 28, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.5126588792104, 'Longitude': -1.287142911440698, 'Elevation': 74.15587122086436}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 30, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.512500728667135, 'Longitude': -1.2870701572958474, 'Elevation': 9.229268110357225}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 31, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.51261661811168, 'Longitude': -1.287386610831244, 'Elevation': 44.06919602677226}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 32, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.51248144569346, 'Longitude': -1.287223865145113, 'Elevation': 54.13878069166094}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 33, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.51250883031841, 'Longitude': -1.2872314159766747, 'Elevation': 60.057777599431574}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 34, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.512514185087724, 'Longitude': -1.2872319452887357, 'Elevation': 60.502179473638535}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 35, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.512514758666946, 'Longitude': -1.287229761382232, 'Elevation': 61.11482547223568}\n{'Time': datetime.datetime(2019, 11, 9, 13, 18, 36, tzinfo=SimpleTZ(\"Z\")), 'Latitude': 47.512514758666946, 'Longitude': -1.287229761382232, 'Elevation': 61.11482547223568}\n"
    }
   ],
   "source": [
    "mypoints = gpsPointList(gpx)\n",
    "for i in range(0,10):\n",
    "    print(mypoints[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2019-11-09 13:18:27+00:00\n2019-11-09 13:18:27+00:00\n2019-11-09 13:18:28+00:00\n2019-11-09 13:18:30+00:00\n2019-11-09 13:18:31+00:00\n2019-11-09 13:18:32+00:00\n2019-11-09 13:18:33+00:00\n2019-11-09 13:18:34+00:00\n2019-11-09 13:18:35+00:00\n2019-11-09 13:18:36+00:00\n"
    }
   ],
   "source": [
    "for i in range (0,10):\n",
    "    print(mypoints[i]['Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pykml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developers.google.com/kml/documentation/kml_tut#placemarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykml import parser\n",
    "root = parser.fromstring(open(kml_path, 'r',encoding=\"utf-8\").read())\n",
    "print (root.Document.Placemark.Point.coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for placemark in root.Document.Placemark:\n",
    "    print(placemark.name)\n",
    "    print(placemark.styleUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "157"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placemark_name = []\n",
    "for placemark in root.Document.Placemark:\n",
    "    placemark_name.append(placemark.name)\n",
    "len(placemark_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for placemark in root.Document.Placemark:\n",
    "    if(placemark.name == 0):\n",
    "        print(placemark.name)\n",
    "        test = placemark\n",
    "\n",
    "for SimpleData in test.ExtendedData.SchemaData.SimpleData:\n",
    "        print(SimpleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\n43.497077\n-1.376946\n289.100006\n1.15\n0\n51.9646\n19/02/2020\n15:12:33\n1.027505\n-9.441706\n0.149936\n0.56892\n"
    }
   ],
   "source": [
    "for SimpleData in test.ExtendedData.SchemaData.SimpleData:\n",
    "        print(SimpleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "placemark_info = []\n",
    "for SimpleData in test.ExtendedData.SchemaData.SimpleData:\n",
    "    placemark_info.append(SimpleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\n43.497077\n-1.376946\n289.100006\n1.15\n0\n51.9646\n19/02/2020\n15:12:33\n1.027505\n-9.441706\n0.149936\n0.56892\n"
    }
   ],
   "source": [
    "for i in range(0,len(placemark_info)):\n",
    "    print(placemark_list[i])\n",
    "# 1, 2, 6, 7, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'Time': '19/02/202015:12:33',\n 'Latitude': 43.497077,\n 'Longitude': -1.376946,\n 'Elevation': 51.9646}"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placemark_dico = {\"Time\":str(placemark_info[7])+str(placemark_info[8]),\"Latitude\":placemark_info[1],\"Longitude\":placemark_info[2],\"Elevation\":placemark_info[6]}\n",
    "placemark_dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "157"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(placemark_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\nVideo start\n1\n0\n2\n1\n3\n2\n4\n3\n5\n4\n6\n5\n7\n6\n8\n7\n9\n8\n10\n9\n11\n10\n12\n11\n13\n12\n14\n14\n15\n15\n16\n17\n17\n18\n18\n20\n19\n22\n20\n23\n21\n24\n22\n25\n23\n26\n24\n27\n25\n28\n26\n29\n27\n30\n28\n31\n29\n32\n30\n33\n31\n34\n32\n35\n33\n36\n34\n37\n35\n38\n36\n39\n37\n40\n38\n41\n39\n42\n40\n43\n41\n45\n42\n47\n43\n49\n44\n51\n45\n53\n46\n55\n47\n57\n48\n59\n49\n61\n50\n63\n51\n65\n52\n67\n53\n69\n54\n71\n55\n73\n56\n75\n57\n77\n58\n78\n59\n79\n60\n80\n61\n81\n62\n82\n63\n83\n64\n84\n65\n85\n66\n86\n67\n87\n68\n88\n69\n89\n70\n90\n71\n91\n72\n92\n73\n93\n74\n94\n75\n95\n76\n96\n77\n97\n78\n98\n79\n99\n80\n100\n81\n101\n82\n102\n83\n103\n84\n104\n85\n105\n86\n106\n87\n107\n88\n109\n89\n110\n90\n111\n91\n113\n92\n114\n93\n115\n94\n117\n95\n119\n96\n120\n97\n122\n98\n123\n99\n124\n100\n125\n101\n126\n102\n127\n103\n128\n104\n129\n105\n131\n106\n133\n107\n134\n108\n135\n109\n137\n110\n138\n111\n139\n112\n140\n113\n141\n114\n142\n115\n143\n116\n144\n117\n145\n118\n147\n119\n149\n120\n151\n121\n152\n122\n153\n123\n154\n124\n155\n125\n156\n126\n157\n127\n158\n128\n159\n129\n160\n130\n161\n131\n162\n132\n163\n133\n164\n134\n166\n135\n167\n136\n168\n137\n169\n138\n170\n139\n171\n140\n173\n141\n174\n142\n175\n143\n176\n144\n177\n145\n178\n146\n180\n147\n181\n148\n182\n149\n184\n150\n186\n151\n188\n152\n190\n153\n191\n154\n193\n155\n194\n156\nVideo end\n"
    }
   ],
   "source": [
    "for i in range(0,len(placemark_name)):\n",
    "    print(i)\n",
    "    print(placemark_name[i])\n",
    "    for placemark in root.Document.Placemark:\n",
    "        if(placemark.name == placemark_name):\n",
    "            print(placemark.name)\n",
    "            temp = placemark\n",
    "            for SimpleData in test.ExtendedData.SchemaData.SimpleData:\n",
    "                print(SimpleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "type(str(placemark_list[7]))\n",
    "placemark_list[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '19/02/2020' does not match format '%Y %m %d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-4ac8700b5331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplacemark_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%Y %m %d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/surfrider/lib/python3.6/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    564\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/surfrider/lib/python3.6/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 362\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n",
      "\u001b[0;31mValueError\u001b[0m: time data '19/02/2020' does not match format '%Y %m %d'"
     ]
    }
   ],
   "source": [
    "datetime.strptime(str(placemark_list[7]),'%Y %m %d')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### azure pre-requesite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get connection_string\n",
    "import os\n",
    "connection_string = os.getenv(\"CONN_STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import container client and enumerate blob within container\n",
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "def blobInContainer(connection_s,container_n):\n",
    "    campaign_container = ContainerClient.from_connection_string(conn_str=connection_s, container_name=container_n)\n",
    "    blob_list = campaign_container.list_blobs()\n",
    "    blob_names_list = []\n",
    "    for blob in blob_list:\n",
    "        blob_names_list.append(blob.name)\n",
    "    return blob_names_list\n",
    "\n",
    "blobs_campaign0 = blobInContainer(connection_string,'campaign0')\n",
    "print(blobs_campaign0)\n",
    "\n",
    "# Get Blob Video info\n",
    "from azure.storage.blob import BlobClient\n",
    "\n",
    "def blobInfos(connection_s,container_n,blob_n):\n",
    "    blob_video = BlobClient.from_connection_string(conn_str=connection_s,container_name=container_n, blob_name=blob_n)\n",
    "    blob_video_url = blob_video.url\n",
    "    blob_video_prop = blob_video.get_blob_properties()\n",
    "    blob_video_prop_keys = blob_video_prop.keys()\n",
    "    print(\"blob name:\",blob_n)\n",
    "    print(\"blob URL:\",blob_video_url)\n",
    "    print(\"blob properties:\", blob_video_prop)\n",
    "    print(\"blob properties keys:\", blob_video_prop_keys)\n",
    "\n",
    "blob_video_name = 'vid1-4K4.mp4'\n",
    "blobInfos(connection_string,'campaign0',blob_video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobClient\n",
    "blob_video0 = BlobClient.from_connection_string(conn_str=connection_string,container_name=\"campaign0\", blob_name=\"vid1-4K4.mp4\")\n",
    "\n",
    "''' function to download blob from Azure to local file system'''\n",
    "''' parameter is a blob client object from azure storage sdk'''\n",
    "def downloadBlob(blobclient):\n",
    "    with open(\"/tmp/\"+blobclient.blob_name, \"wb\") as my_blob_dl:\n",
    "        blob_data = blobclient.download_blob()\n",
    "        blob_data.readinto(my_blob_dl)\n",
    "    print(\"Blob %s downloaded\" %blobclient.blob_name)\n",
    "    print(\"Blob path: /tmp/%s\" %blobclient.blob_name)\n",
    "\n",
    "downloadBlob(blob_video0)\n",
    "# this function should handle connection issue"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REST requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/3777301/how-to-call-a-shell-script-from-python-code\n",
    "#https://stackoverflow.com/questions/325463/launch-a-shell-command-with-in-a-python-script-wait-for-the-termination-and-ret\n",
    "\n",
    "import subprocess\n",
    "\n",
    "''' getPrediction function sends curl request to Surfrider AI'''\n",
    "''' video name is the name of a locally downloaded video from Azure'''\n",
    "''' video name is passed as an argument to curl_request script '''\n",
    "''' curl_request script sends actual POST request to Surfrider AI'''\n",
    "def getPrediction(video_name):\n",
    "    curl_request_script = ['./curl_request_param.sh',video_name]\n",
    "    output = []\n",
    "    request_answer = subprocess.Popen(curl_request_script, stdout=subprocess.PIPE)\n",
    "    for line in request_answer.stdout:\n",
    "        print(line)\n",
    "        output.append(line)\n",
    "    return output\n",
    "\n",
    "# this function should handle connection issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from AI on vid1-4K4.MP4\n",
    "prediction = getPrediction('vid1-4K4.mp4')\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JSON Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "''' jsonPrediction cast a prediction from getPrediction function '''\n",
    "''' and cast the list prediction as a string then dictionnary with json_loads '''\n",
    "def jsonPrediction(pred):\n",
    "    string_prediction = str(pred[0])[2:-3] #removing 2 x first and 3 last characters of pred\n",
    "    json_prediction = json.loads(string_prediction)\n",
    "    return json_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prediction = jsonPrediction(prediction)\n",
    "json_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_to_box in json_prediction['detected_trash']:\n",
    "    print(frame_to_box)\n",
    "    print(frame_to_box['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' getTrashLabel return label from a frame_to_box'''\n",
    "def getTrashLabel(frame_2_box):\n",
    "    return frame_to_box['label']\n",
    "\n",
    "test_f2b = {'frame_to_box': {'2': [0.45, 0.44, 0.5, 0.5]}, 'id': 0, 'label': 'fragments'}\n",
    "getTrashLabel(test_f2b['frame_to_box'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### requests - skip for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make first GET Request using requests module\n",
    "import requests\n",
    "r_get = requests.get('https://github.com/timeline.json')\n",
    "print(r_get.text)\n",
    "# Make first POST Request using requests module\n",
    "r_post = requests.post(\"http://httpbin.org/post\")\n",
    "print(r_post.text) # response in text format\n",
    "print(r_post.json()) # response is json formated\n",
    "\n",
    "# Video POST request example\n",
    "desktop = '/Users/clement/Desktop/'\n",
    "video = 'vid1-4K.MP4'\n",
    "video_path = os.path.join(desktop,video)\n",
    "url = 'http://aiapisurfrider.northeurope.cloudapp.azure.com:5000'\n",
    "myfile = {'file': open(video_path ,'rb')}\n",
    "#myairequest = requests.post(url, files = myfile)\n",
    "\n",
    "# Image POST request example\n",
    "desktop = '/Users/clement/Desktop/'\n",
    "img = 'G0061735.JPG'\n",
    "img_path = os.path.join(desktop,img)\n",
    "url = 'http://aiapisurfrider.northeurope.cloudapp.azure.com:5000'\n",
    "myfile = {'file': open(img_path ,'rb')}\n",
    "myairequest = requests.post(url, files = myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sql odbc driver and connect to SQL DB\n",
    "import os\n",
    "import pyodbc\n",
    "server = os.getenv(\"SERVER\")\n",
    "database = os.getenv(\"DATABASE\")\n",
    "username = os.getenv(\"USERNAME\")\n",
    "password = os.getenv(\"PWD\")\n",
    "driver= '{ODBC Driver 17 for SQL Server}'\n",
    "cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Query on dbo.Trash_Type\n",
    "cursor.execute(\"SELECT * FROM dbo.Trash_Type\")\n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "    print (str(row[0]))\n",
    "    print (str(row[1]))\n",
    "    row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLabel2TrashId(label):\n",
    "    switcher = { \n",
    "    \"Fishing or Hunting\":\"89B44BAA-69AA-4109-891A-128E012E7E07\",\n",
    "    \"Food Packaging\":\"185FEFA2-EEF2-47A8-873E-26032A4BB3C3\",\n",
    "    \"Unknown\":\"BB4DEA69-218A-40CC-A000-2AE17C37152C\",\n",
    "    \"Industrial or Construction Debris\":\"2A863E38-E5D0-455F-87CE-2B75DA29F59A\",\n",
    "    \"fragments\":\"ED401B92-DC24-44C0-A52A-34CE831092BF\",\n",
    "    \"Agricultural Waste\":\"36B2AFEB-7A7C-44B5-A790-5E5C73BA144D\",\n",
    "    \"others\":\"4BEC18FC-BC48-45B7-AFDA-6BA96BD80921\",\n",
    "    \"Common Household Items\":\"C68E90CF-6E65-4474-BC60-72E1C8513F55\",\n",
    "    \"plastic\":\"6961D0DB-928C-419E-9985-98EEEAF552C7\",\n",
    "    \"bottles\":\"9780940B-D06C-4AAB-8003-AB914981E87A\",\n",
    "    \"Drinking Bottles\":\"BCF549A8-AECD-4BC9-B9B8-B94A8F3758D5\",\n",
    "    \"Unknown10\":\"BC7BB564-BE04-4B4B-9913-FF69780B93A6\"\n",
    "    } \n",
    "    return switcher.get(label, \"nothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trash INSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' trashInsert execute INSERT request to dbo.Trash table'''\n",
    "''' parameter to indsert is a trashTypeId defined in Trash_Type table'''\n",
    "def trashInsert(trashTypeId,cursor,cnxn):\n",
    "    cursor.execute(\"INSERT dbo.Trash (Id,CampaignId,Latitude,Longitude,TrashTypeId,Precision,AI_Version) VALUES (NEWID(),'8D21A132-CF4B-404E-B287-C40A2F12D305','50.797731', '2.179029', ? ,'0.95','0')\",trashTypeId)\n",
    "    cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing INSERT for all trash detected in json_prediction from vid1-4K4.MP4\n",
    "for frame_to_box in json_prediction['detected_trash']:\n",
    "    print(frame_to_box)\n",
    "    trashLabel = getTrashLabel(frame_to_box)\n",
    "    print(trashLabel)\n",
    "    mapLabel = mapLabel2TrashId(trashLabel)\n",
    "    print(mapLabel)\n",
    "    trashInsert(mapLabel,cursor,cnxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trash End to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "from azure.storage.blob import BlobClient\n",
    "import json \n",
    "import os\n",
    "import pyodbc\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' blobContainer create a name_list of blobs within container'''\n",
    "''' params are container name (no full url) & storage conn string'''\n",
    "def blobInContainer(connection_s,container_n):\n",
    "    campaign_container = ContainerClient.from_connection_string(conn_str=connection_s, container_name=container_n)\n",
    "    blob_list = campaign_container.list_blobs()\n",
    "    blob_names_list = []\n",
    "    for blob in blob_list:\n",
    "        blob_names_list.append(blob.name)\n",
    "    return blob_names_list\n",
    "\n",
    "''' blobInfos provides basic information about a blob object'''\n",
    "''' params are blob_name only (no full url) & storage conn string'''\n",
    "def blobInfos(connection_s,container_n,blob_n):\n",
    "    blob_video = BlobClient.from_connection_string(conn_str=connection_s,container_name=container_n, blob_name=blob_n)\n",
    "    blob_video_url = blob_video.url\n",
    "    blob_video_prop = blob_video.get_blob_properties()\n",
    "    blob_video_prop_keys = blob_video_prop.keys()\n",
    "    print(\"blob name:\",blob_n)\n",
    "    print(\"blob URL:\",blob_video_url)\n",
    "    print(\"blob properties:\", blob_video_prop)\n",
    "    print(\"blob properties keys:\", blob_video_prop_keys)\n",
    "\n",
    "\n",
    "''' downloadBlob from Azure to local file system'''\n",
    "''' parameter is a blob client object from azure storage sdk'''\n",
    "def downloadBlob(blobclient):\n",
    "    with open(\"/tmp/\"+blobclient.blob_name, \"wb\") as my_blob_dl:\n",
    "        blob_data = blobclient.download_blob()\n",
    "        blob_data.readinto(my_blob_dl)\n",
    "    print(\"Blob %s downloaded\" %blobclient.blob_name)\n",
    "    print(\"Blob path: /tmp/%s\" %blobclient.blob_name)\n",
    "\n",
    "\n",
    "''' getPrediction function sends curl request to Surfrider AI'''\n",
    "''' video name is the name of a locally downloaded video from Azure'''\n",
    "''' video name is passed as an argument to curl_request script '''\n",
    "''' curl_request script sends actual POST request to Surfrider AI'''\n",
    "def getPrediction(video_name):\n",
    "    curl_request_script = ['./curl_request_param.sh',video_name]\n",
    "    output = []\n",
    "    request_answer = subprocess.Popen(curl_request_script, stdout=subprocess.PIPE)\n",
    "    i = 0\n",
    "    for line in request_answer.stdout:\n",
    "        print(line)\n",
    "        output.append(line)\n",
    "    return output\n",
    "\n",
    "\n",
    "''' jsonPrediction cast a prediction from getPrediction function '''\n",
    "''' from a list prediction to a string then dictionnary with json_loads '''\n",
    "def jsonPrediction(pred):\n",
    "    string_prediction = str(pred[0])[2:-3] #removing 2 x first and 3 last characters of pred\n",
    "    json_prediction = json.loads(string_prediction)\n",
    "    return json_prediction\n",
    "\n",
    "\n",
    "''' getTrashLabel return label from a frame_to_box'''\n",
    "def getTrashLabel(frame_2_box):\n",
    "    return frame_2_box['label']\n",
    "\n",
    "''' mapLabelTrashId is a switch that converts label to TrashId'''\n",
    "''' param is label that comes from AI predictions dictionnary jsonPrediction'''\n",
    "def mapLabel2TrashId(label):\n",
    "    switcher = { \n",
    "    \"Fishing or Hunting\":\"89B44BAA-69AA-4109-891A-128E012E7E07\",\n",
    "    \"Food Packaging\":\"185FEFA2-EEF2-47A8-873E-26032A4BB3C3\",\n",
    "    \"Unknown\":\"BB4DEA69-218A-40CC-A000-2AE17C37152C\",\n",
    "    \"Industrial or Construction Debris\":\"2A863E38-E5D0-455F-87CE-2B75DA29F59A\",\n",
    "    \"fragments\":\"ED401B92-DC24-44C0-A52A-34CE831092BF\",\n",
    "    \"Agricultural Waste\":\"36B2AFEB-7A7C-44B5-A790-5E5C73BA144D\",\n",
    "    \"others\":\"4BEC18FC-BC48-45B7-AFDA-6BA96BD80921\",\n",
    "    \"Common Household Items\":\"C68E90CF-6E65-4474-BC60-72E1C8513F55\",\n",
    "    \"plastic\":\"6961D0DB-928C-419E-9985-98EEEAF552C7\",\n",
    "    \"bottles\":\"9780940B-D06C-4AAB-8003-AB914981E87A\",\n",
    "    \"Drinking Bottles\":\"BCF549A8-AECD-4BC9-B9B8-B94A8F3758D5\",\n",
    "    \"Unknown10\":\"BC7BB564-BE04-4B4B-9913-FF69780B93A6\"\n",
    "    } \n",
    "    return switcher.get(label, \"nothing\")\n",
    "\n",
    "\n",
    "''' trashInsert execute INSERT request to dbo.Trash table'''\n",
    "''' parameter to indsert is a trashTypeId defined in Trash_Type table'''\n",
    "def trashInsert(cursor,cnxn,trashTypeId):\n",
    "    cursor.execute(\"INSERT dbo.Trash (Id,CampaignId,Latitude,Longitude,TrashTypeId,Precision,AI_Version) VALUES (NEWID(),'8D21A132-CF4B-404E-B287-C40A2F12D305','50.797731', '2.179029', ? ,'0.95','0')\",trashTypeId)\n",
    "    cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # blob storage connection string\n",
    "    connection_string = os.getenv(\"CONN_STRING\")\n",
    " \n",
    "    # get list of blobs in container campaign0\n",
    "    campaign_container_name = 'campaign0'\n",
    "    #campaign_container_name = 'campaign1'\n",
    "    blobs_campaign0 = blobInContainer(connection_string,campaign_container_name)\n",
    "    print(blobs_campaign0)\n",
    "\n",
    "    # get infos of blob video vid1-4K.MP4,vid1-4K2.MP4,vid1-4K3.mp4,vid1-4K4.mp4,19022020_Adour_6.mp4\n",
    "#    blob_video_name = '19022020_Adour_6.mp4'\n",
    "    blob_video_name = 'vid1-4K4.mp4'   \n",
    "    blobInfos(connection_string,campaign_container_name,blob_video_name)\n",
    "\n",
    "    # download blob video\n",
    "    blob_video0 = BlobClient.from_connection_string(conn_str=connection_string,container_name=campaign_container_name, blob_name=blob_video_name)\n",
    "    downloadBlob(blob_video0)\n",
    "\n",
    "    # get predictions from AI on vid1-4K4.MP4\n",
    "    prediction = getPrediction(blob_video_name)\n",
    "\n",
    "    # cast prediction to JSON/Dictionnary format\n",
    "    json_prediction = jsonPrediction(prediction)\n",
    "    json_prediction\n",
    "\n",
    "    # SQL connection\n",
    "    server = os.getenv(\"SERVER\")\n",
    "    database = os.getenv(\"DATABASE\")\n",
    "    username = os.getenv(\"USERNAME\")\n",
    "    password = os.getenv(\"PWD\")\n",
    "    driver= '{ODBC Driver 17 for SQL Server}'\n",
    "    cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    # Executing INSERT for all trash detected in json_prediction from vid1-4K4.MP4\n",
    "    for frame_to_box in json_prediction['detected_trash']:\n",
    "        print(frame_to_box)\n",
    "        trashLabel = getTrashLabel(frame_to_box)\n",
    "        print(trashLabel)\n",
    "        mapLabel = mapLabel2TrashId(trashLabel)\n",
    "        print(mapLabel)\n",
    "        trashInsert(cursor,cnxn,mapLabel)\n",
    "    \n",
    "    # Visualize main result within dbo.Trash Table\n",
    "    cursor.execute(\"SELECT * FROM dbo.Trash\")\n",
    "    row = cursor.fetchone()\n",
    "    while row:\n",
    "        print (str(row[0]))\n",
    "        row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL Ops - Optional check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Prequesite\n",
    "server = os.getenv(\"SERVER\")\n",
    "database = os.getenv(\"DATABASE\")\n",
    "username = os.getenv(\"USERNAME\")\n",
    "password = os.getenv(\"PWD\")\n",
    "driver= '{ODBC Driver 17 for SQL Server}'\n",
    "cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT all record from dbo.Trash\n",
    "cursor.execute(\"SELECT * FROM dbo.Trash\")\n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "    for i in range(0,5):\n",
    "        print (str(row[i]))\n",
    "    row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE ALL RECORD from Trash table\n",
    "cursor.execute(\"DELETE FROM dbo.Trash\")\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}